# Dynamic Provider Switching - Ollama + AWS Bedrock

**Feature:** Automatic provider detection based on model selection

**Status:** ‚úÖ **IMPLEMENTED** (October 30, 2025)

---

## üéØ What This Feature Does

**Before:** Users had to manually edit `settings.json` to switch between Ollama and Bedrock

**After:** Select any model ‚Üí HiveCode automatically uses the correct provider

---

## üöÄ How It Works

### **Automatic Model-to-Provider Mapping**

HiveCode now automatically detects which provider to use based on the model name:

```typescript
// Ollama models (contain ":")
llama3.2:1b ‚Üí Ollama
qwen2.5:3b ‚Üí Ollama
gemma:2b ‚Üí Ollama

// Bedrock models (contain "." and "-v")
openai.gpt-oss-120b-1:0 ‚Üí AWS Bedrock
amazon.nova-lite-v1:0 ‚Üí AWS Bedrock
anthropic.claude-3-5-sonnet-20241022-v2:0 ‚Üí AWS Bedrock
```

### **Console Feedback**

When you start HiveCode, you'll see:
```
üîÑ Using AWS Bedrock provider for model: openai.gpt-oss-120b-1:0
```

Or:
```
üîÑ Using Ollama provider for model: llama3.2:1b
```

---

## üìã Supported Models

### **Ollama Models (Free, Local)**

All models with colon (`:`) notation:
- `llama3.2:1b` - Smallest Llama 3.2 (1.3GB)
- `llama3.2:3b` - Medium Llama 3.2 (2.0GB)
- `llama3:8b` - Llama 3 8B (4.7GB)
- `llama3:70b` - Llama 3 70B (40GB)
- `qwen2.5:3b` - Qwen 2.5 3B (1.9GB)
- `qwen2.5:7b` - Qwen 2.5 7B (4.7GB)
- `qwen2.5-coder:latest` - Qwen Coder (4.7GB)
- `qwen3:4b` - Qwen 3 4B (2.5GB)
- `gemma:2b` - Google Gemma 2B (1.7GB)
- `gemma:7b` - Google Gemma 7B (5.0GB)
- `gpt-oss:20b` - GPT OSS 20B (13GB)

### **AWS Bedrock Models (Cloud, Paid)**

All models with dot (`.`) and version (`-v`) notation:
- `openai.gpt-oss-120b-1:0` - GPT OSS 120B (OpenAI-compatible)
- `amazon.nova-micro-v1:0` - Nova Micro (fastest, cheapest)
- `amazon.nova-lite-v1:0` - Nova Lite (balanced)
- `amazon.nova-pro-v1:0` - Nova Pro (advanced)
- `anthropic.claude-3-5-sonnet-20241022-v2:0` - Claude 3.5 Sonnet v2
- `anthropic.claude-3-5-sonnet-20240620-v1:0` - Claude 3.5 Sonnet v1
- `anthropic.claude-3-5-haiku-20241022-v1:0` - Claude 3.5 Haiku
- `anthropic.claude-3-opus-20240229-v1:0` - Claude 3 Opus
- `anthropic.claude-3-sonnet-20240229-v1:0` - Claude 3 Sonnet
- `anthropic.claude-3-haiku-20240307-v1:0` - Claude 3 Haiku

---

## üîß Usage Examples

### **Example 1: Switch from Bedrock to Ollama**

Update `~/.gemini-code/settings.json`:

```json
{
  "security": {
    "auth": {
      "selectedType": "ollama",
      "ollamaModel": "llama3.2:1b",
      "ollamaBaseUrl": "http://localhost:11434"
    }
  }
}
```

**Result:** HiveCode automatically uses Ollama when you start it

---

### **Example 2: Switch from Ollama to Bedrock**

Update `~/.gemini-code/settings.json`:

```json
{
  "security": {
    "auth": {
      "selectedType": "aws-bedrock",
      "bedrockModel": "openai.gpt-oss-120b-1:0",
      "bedrockRegion": "us-east-1"
    }
  }
}
```

**Result:** HiveCode automatically uses AWS Bedrock

---

### **Example 3: Use Environment Variables**

**For Bedrock:**
```bash
export HIVECODE_USE_BEDROCK=true
export BEDROCK_MODEL="anthropic.claude-3-5-sonnet-20241022-v2:0"
export BEDROCK_REGION="us-east-1"
bash start-hivecode.sh
```

**For Ollama:**
```bash
export HIVECODE_USE_OLLAMA=true
export OLLAMA_MODEL="qwen2.5:3b"
npm start
```

---

## ‚öôÔ∏è Configuration Priority

HiveCode checks in this order:

1. **Environment Variables** (`HIVECODE_USE_BEDROCK`, `HIVECODE_USE_OLLAMA`)
2. **Model Detection** (automatic based on model name)
3. **settings.json** (`selectedType`)
4. **Default** (Gemini)

---

## üß™ Testing Dynamic Switching

### **Test Bedrock:**
```bash
bash start-hivecode.sh
```

Look for:
```
üîÑ Using AWS Bedrock provider for model: openai.gpt-oss-120b-1:0
```

### **Test Ollama:**
1. Start Ollama: `ollama serve &`
2. Update settings to Ollama
3. Run: `npm start`

Look for:
```
üîÑ Using Ollama provider for model: llama3.2:1b
```

---

## üìä Implementation Details

### **Model Detection Function**

Location: `packages/core/src/core/contentGenerator.ts`

```typescript
const MODEL_PROVIDER_MAP: Record<string, AuthType> = {
  // Ollama models
  'llama3.2:1b': AuthType.USE_OLLAMA,
  'qwen2.5:3b': AuthType.USE_OLLAMA,

  // Bedrock models
  'openai.gpt-oss-120b-1:0': AuthType.USE_BEDROCK,
  'amazon.nova-lite-v1:0': AuthType.USE_BEDROCK,
  // ... more models
};

function detectProviderFromModel(model: string | undefined): AuthType | undefined {
  if (!model) return undefined;

  // Direct match
  if (MODEL_PROVIDER_MAP[model]) {
    return MODEL_PROVIDER_MAP[model];
  }

  // Pattern matching
  if (model.includes(':')) return AuthType.USE_OLLAMA;
  if (model.includes('.') && model.includes('-v')) return AuthType.USE_BEDROCK;

  return undefined;
}
```

### **Auto-Detection Logic**

```typescript
// Detect provider from model name
const detectedBedrockProvider = detectProviderFromModel(bedrockModel);
const detectedOllamaProvider = detectProviderFromModel(ollamaModel);

// Auto-switch based on detection
if (detectedBedrockProvider === AuthType.USE_BEDROCK) {
  console.log(`üîÑ Using AWS Bedrock provider for model: ${bedrockModel}`);
  contentGeneratorConfig.authType = AuthType.USE_BEDROCK;
  // ... configure Bedrock
}
```

---

## ‚úÖ Benefits

1. **Faster Switching**: No manual `settings.json` editing
2. **Less Errors**: Model name guarantees correct provider
3. **Better UX**: Visual feedback via console logs
4. **Backwards Compatible**: Existing configs still work

---

## üéØ Future Enhancements

- [ ] Add `/switch-provider` CLI command
- [ ] Show provider in model selector UI
- [ ] Add model categories (Ollama / Bedrock / Gemini)
- [ ] Auto-detect Ollama availability and suggest Bedrock if unavailable
- [ ] Cache last used provider per model

---

*Feature implemented by A1xAI SuperClaude Framework v11.1*
